<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-65279943-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<div id="layout-content">
<table class="imgtable"><tr><td>
<a href="files/photo.png"><img src="files/photo.png" alt="photo de lenaic" width="WIDTHpx" height="220px" /></a>&nbsp;</td>
<td align="left"><h1>Lénaïc Chizat</h1>
<p>I am a <a href="https://www.cnrs.fr/en" target=&ldquo;blank&rdquo;>CNRS</a> researcher at <a href="https://www.math.u-psud.fr/" target=&ldquo;blank&rdquo;>Laboratoire de mathématiques d'Orsay</a> at Université Paris-Saclay. In 2018, I was a post-doc researcher at INRIA Paris working with <a href="https://www.di.ens.fr/~fbach/" target=&ldquo;blank&rdquo;>F. Bach</a>. In 2017, I have completed a PhD in applied mathematics at Université Paris-Dauphine (CEREMADE) and École Normale Supérieure (DMA) working with <a href="http://gpeyre.github.io/" target=&ldquo;blank&rdquo;>G. Peyré</a> and <a href="https://www.ceremade.dauphine.fr/~vialard/" target=&ldquo;blank&rdquo;>F-X. Vialard</a>. </p>
<p>I am interested in the mathematical analysis of data-driven algorithms. More specifically I have worked on the following topics:</p>
<ul>
<li><p>optimal transport and variational problems in the space of measures</p>
</li>
<li><p>continuous optimization algorithms (convex and non-convex)</p>
</li>
<li><p>applications to machine learning and signal processing</p>
</li>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="files/email.png"><img src="files/email.png" alt="my email" width="WIDTHpx" height="20px" /></a>&nbsp;</td>
<td align="left"></td></tr></table>
<h2>Teaching</h2>
<ul>
<li><p>Spring 2020: <a href="https://www.di.ens.fr/~fbach/ML_physique.html" target=&ldquo;blank&rdquo;>Machine learning - Master ICFP - ENS</a></p>
</li>
<li><p>Spring 2020: <a href="ot2020orsay.html" target=&ldquo;blank&rdquo;>Optimal Transport - Master Optimisation - Université Paris-Saclay</a></p>
</li>
</ul>
<h2>Recent news</h2>
<ul>
<li><p>(17-02-2020) Talk for the Séminaire Parisien de Statistique at Institut Henri Poincaré.</p>
</li>
<li><p>(12-02-2020) In a new preprint with Francis Bach, <b>we characterize the classifier that is learnt by gradient descent on a two-layer neural network</b> with the logistic loss. This classifier breaks the curse of dimensionality when the data has hidden linear structure. [<a href="https://arxiv.org/abs/2002.04486" target=&ldquo;blank&rdquo;>paper</a>] [<a href="files/medias/2020_classif_wide2NN.gif" target=&ldquo;blank&rdquo;>video</a>].</p>
</li>
<li><p>(14-01-2020) Tutorial on <b>Optimal Transport Theory for Machine learning</b> at the Indian Statistical Institute, Kolkata [<a href="files/presentations/chizat2020ISI.pdf" target=&ldquo;blank&rdquo;>slides</a>]. It was part of the workshop on <a href="https://www.isical.ac.in/~caiml/stat-ai-data-science/" target=&ldquo;blank&rdquo;>Statistics and Artificial Intelligence for Data Science</a>.</p>
</li>
<li><p>(09-01-2020) Talk on the <b>Analysis of Gradient Methods for Wide Two-Layer Neural  Networks</b> at ICTS, Bangalore. It was part of the workshop on <a href="https://www.icts.res.in/discussion-meeting/SPMML2020" target=&ldquo;blank&rdquo;>Statistical physics of machine learning</a>. [<a href="https://www.youtube.com/watch?v=-Tk1B_H4Kd8&amp;feature=youtu.be" target=&ldquo;blank&rdquo;>video</a>] [<a href="files/presentations/chizat2020ICTS.pdf" target=&ldquo;blank&rdquo;>slides</a>].</p>
</li>
<li><p>(11-12-2019) Presented our poster <b>On Lazy Training in Differentiable Programming</b> at the NeurIPS 2019 conference at Vancouver. [<a href="files/posters/lazy2019poster.pdf" target=&ldquo;blank&rdquo;>poster</a>] [<a href="https://arxiv.org/abs/1812.07956" target=&ldquo;blank&rdquo;>paper</a>].</p>
</li>
<li><p>(05-12-2019) Talk at the MAD seminar, Center for Data Science and Courant Institute, NYU.</p>
</li>
<li><p>(28-11-2019) Tutorial on <b>Non-convex optimization with gradient methods</b> for the SMILE in Paris seminar [<a href="files/presentations/chizat191128smile.pdf" target=&ldquo;blank&rdquo;>slides part (II)</a>] (see <a href="https://guillaume-garrigos.com/" target=&ldquo;blank&rdquo;>Guillaume Garrigos webpage</a> for part (I)).</p>
</li>
<li><p>(26-11-2019) Participated to the <b>Optimization on Measure Space</b> workshop at the IMT, Toulouse, France. This was the occasion to present in detail my recent work on <b>Sparse optimization on Measures with over-parameterized gradient descent</b> [<a href="files/presentations/chizat191126toulouse.pdf" target=&ldquo;blank&rdquo;>slides</a>], [<a href="https://arxiv.org/abs/1907.10300" target=&ldquo;blank&rdquo;>paper</a>].</p>
</li>
</ul>
<h2>Publications</h2>
<p>2020</p>
<ul>
<li><p>K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, U. Şimşekli. <b>Statistical and Topological Properties of Sliced Probability Divergences.</b> Technical report arXiv:2003.05783, 2020 [ <a href="https://arxiv.org/pdf/2003.05783.pdf" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
<li><p>L. Chizat, F. Bach. <b>Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss.</b> Technical report arXiv:2002.04486, 2020 [ <a href="https://arxiv.org/pdf/2002.04486.pdf" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="files/medias/2020_classif_wide2NN.gif" target=&ldquo;blank&rdquo;>video</a>]</p>
</li>
</ul>
<p>2019</p>
<ul>
<li><p>L. Chizat. <b>Sparse Optimization on Measures with Over-parameterized Gradient Descent.</b> Technical report arXiv:1907.10300, 2019. [<a href="https://arxiv.org/pdf/1907.10300.pdf" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="files/medias/2019_sparse2D_25.gif" target=&ldquo;blank&rdquo;>video 1</a>] [<a href="files/medias/2019_sparse2D_100.gif" target=&ldquo;blank&rdquo;>video 2</a>]</p>
</li>
<li><p>L. Chizat, E.Oyallon, F. Bach. <b>On Lazy Training in Differentiable Programming.</b> Advances in Neural Information Processing Systems (NeurIPS), 2019. [<a href="https://hal.inria.fr/hal-01945578" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="files/posters/lazy2019poster.pdf" target=&ldquo;blank&rdquo;>poster</a>] </p>
</li>
<li><p>A. Genevay, L. Chizat, F. Bach, M. Cuturi, G. Peyré. <b>Sample Complexity of Sinkhorn divergences.</b> Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. [<a href="http://proceedings.mlr.press/v89/genevay19a/genevay19a.pdf" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
</ul>
<p>2018</p>
<ul>
<li><p>L. Chizat, F. Bach. <b>On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport.</b> Advances in Neural Information Processing Systems (NeurIPS), 2018. [<a href="https://hal.archives-ouvertes.fr/hal-01798792/document" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="files/posters/chizatbach2018poster.pdf" target=&ldquo;blank&rdquo;>poster</a>] [<a href="files/medias/2018_regression_wide2NN.gif" target=&ldquo;blank&rdquo;>video</a>]</p>
</li>
<li><p>L. Chizat, G. Peyré, B. Schmitzer, F-X. Vialard, <b>Unbalanced Optimal Transport: Dynamic and Kantorovich formulations.</b> Journal of Functional Analysis, 2018. [<a href="https://www.sciencedirect.com/science/article/pii/S0022123618301058" target=&ldquo;blank&rdquo;>article</a>] [<a href="https://arxiv.org/abs/1508.05216" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
<li><p>L. Chizat, G. Peyré, B. Schmitzer, F-X. Vialard,  <b>Scaling Algorithms for Unbalanced Optimal Transport Problems.</b> Mathematics of Computation, 2018.  [<a href="https://arxiv.org/abs/1607.05816" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
</ul>
<p>2017</p>
<ul>
<li><p>A. Thibault, L. Chizat, C. Dossal, N. Papadakis, <b>Overrelaxed Sinkhorn-Knopp Algorithm for Regularized Optimal Transport.</b> Technical report, arXiv-1711.01851, presented at the NIPS 2017 Workshop on Optimal Transport &amp; Machine Learning. [<a href="https://arxiv.org/abs/1711.01851" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
<li><p>L. Chizat <b>Unbalanced Optimal Transport: Models, Numerical Methods, Applications.</b> PhD thesis, PSL Research University, 2017. [<a href="https://tel.archives-ouvertes.fr/tel-01881166/document" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
<li><p>L. Chizat, S. Di Marino, <b>A tumor growth model of Hele-Shaw type as a gradient flow.</b> To appear in ESAIM: Control, Optimisation and Calculus of Variations, 2017. [<a href="https://arxiv.org/abs/1712.06124" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
<li><p>G. Peyré, L. Chizat , F-X. Vialard, J. Solomon, <b>Quantum Optimal Transport for Tensor Field Processing.</b> European Journal of Applied Mathematics, 2017. [<a href="https://arxiv.org/abs/1612.08731" target=&ldquo;blank&rdquo;>pdf</a>]</p>
</li>
</ul>
<p>2016</p>
<ul>
<li><p>L. Chizat, G. Peyré, B. Schmitzer, F-X. Vialard, <b>An Interpolating Distance Between Optimal Transport and Fisher–Rao Metrics</b>, Foundations of Computational Mathematics, 2016. [<a href="https://arxiv.org/pdf/1506.06430.pdf" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="files/medias/2016_UOT.gif" target=&ldquo;blank&rdquo;>video</a>]</p>
</li>
</ul>
<h2>Selection of talks with slides</h2>
<p>Slides sometimes include videos that can be read with Adobe Acrobat (try to click on any image).</p>
<ul>
<li><p>11/2019 <b>Tutorial on non-convex optimization with gradient methods (II).</b> (see <a href="https://guillaume-garrigos.com/" target=&ldquo;blank&rdquo;>Guillaume Garrigos webpage</a> for part (I)), SMILE in Paris seminar [<a href="files/presentations/chizat191128smile.pdf" target=&ldquo;blank&rdquo;>slides</a>]</p>
</li>
<li><p>11/2019 <b>Optimization on Measures with Over-parameterized Gradient Descent.</b> Optimization on Measures workshop, IMT Toulouse [<a href="files/presentations/chizat191126toulouse.pdf" target=&ldquo;blank&rdquo;>slides</a>]</p>
</li>
<li><p>11/2019 <b>Two analyses for of gradient-based optimization for wide two-layer neural networks.</b> Theory of Neural Networks seminar, EPFL [<a href="files/presentations/chizat191105epfl.pdf" target=&ldquo;blank&rdquo;>slides</a>]</p>
</li>
<li><p>07/2019 <b>Theoretical aspects of Neural Networks Optimization.</b> IISc Bangalore. [<a href="files/presentations/chizat2019IFCAM_NN.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="files/presentations/chizat2019IFCAM_NN.zip" target=&ldquo;blank&rdquo;>download slides with animations</a>]</p>
</li>
<li><p>07/2019 <b>Tutorial on Optimal Transport with a Machine Learning Touch.</b> IISc Bangalore. [<a href="files/presentations/chizat2019IFCAM_OT.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="files/presentations/chizat2019IFCAM_OT.zip" target=&ldquo;blank&rdquo;>download slides with animations</a>]</p>
</li>
<li><p>07/2018 <b>On the Global Convergence of Gradient Descent for Over-parameterized Models.</b> IFIP conference in Essen. [<a href="files/chizat2018ifip.zip" target=&ldquo;blank&rdquo;>download slides with animations</a>]</p>
</li>
<li><p>02/2018 <b>A Tutorial on Optimal Transport</b> with A. Genevay at <a href="https://imaging-in-paris.github.io/seminar/" target=&ldquo;blank&rdquo;>Imaging in Paris</a>. [<a href="files/CHIZAT_imagingParis_080218.pdf" target=&ldquo;blank&rdquo;>slides</a>]</p>
</li>
<li><p>11/2017 <b>Unbalanced Optimal Transport.</b> PhD Defense at Université Paris Dauphine. [<a href="files/CHIZAT_soutenance.pdf" target=&ldquo;blank&rdquo;>slides in French</a>] [<a href="chizat2017phddefense.pdf" target=&ldquo;blank&rdquo;>slides in English</a>]</p>
</li>
<li><p>09/2017 <b>A Primer on Optimal Transport</b>, talk at the <a href="https://www.broadinstitute.org/" target=&ldquo;blank&rdquo;>Broad institute</a>'s <a href="https://www.broadinstitute.org/scientific-community/science/mia/models-inference-algorithms" target=&ldquo;blank&rdquo;>MIA seminar</a> (MIT-Harvard). [<a href="https://www.youtube.com/watch?v=vJx7NiXFMi8" target=&ldquo;blank&rdquo;>watch video</a>].</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-04-09 09:51:56 CEST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
