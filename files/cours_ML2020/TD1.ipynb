{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD1: Least squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(adapted from Pierre Gaillard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear and polynomial least squares regression: curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(X,Y)$ be a pair of real random variable such that $X$ is uniform on $[0,1]$ and $Y = f_*(X)+\\sigma Z$, where $f_*(x) = \\sin(6x)$, $\\sigma = 0.5$, and $Z$ is some is a standard Gaussian random variable, independent from $X$. \n",
    "\n",
    "(1) Generate $n = 40$ realizations $(x_i, y_i), i = 1, \\dots n$ of $(X,Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Plot the realizations of $(X,Y)$, along with the curve $y = f_*(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we try to learn the function $f_*$ from the $n$ samples. We start with empirical risk minimization over the set of linear functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) What are here the input space $\\mathcal{X}$ of the linear regression? What is the output space $\\mathcal{Y}$? What is the optimal predictor among all $L^2$ functions $f:\\mathcal{X} \\to \\mathcal{Y}$? (Here optimal means that it minimizes the risk $R(f)$.) What is the risk of the optimal predictor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical risk minimization over the set of linear function means that we estimate\n",
    "$$ \\hat{f} = {\\rm argmin}_{f \\in F} \\hat{R}(f) $$ \n",
    "where \n",
    "$$ F = \\{f(x) = w_0 + w_1 x | w_0, w_1 \\in \\mathbb{R}\\} \\, , $$\n",
    "$$ \\hat{R}(f) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f(x_i))^2 \\, .$$\n",
    "\n",
    "(4) Writing $\\hat{f}(x) = w_1 x + w_0$, compute $w_0$, $w_1$ in terms of the observations $(x_i,y_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Using this formula, complete the previous plot with this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Repeat the computation of the coefficients, using now the function `numpy.linalg.lstsq` . Check that the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Minimize the empirical risk over the set of polynomials of order 2. Plot the optimal polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Generalize your code in order to compute the optimal polynomial of order $k$. Vary $k$ and the number of samples $n$, and plot the results. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) Let us denote $\\hat{f}_k$ the minimizer of the empirical risk over the polynomials of order $k$. Plot the risk $R(\\hat{f}_k)$ and the empirical risk $\\hat{R}(\\hat{f}_k)$ as a function of $k$, for $n=40$ and $n=400$. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider problems in higher dimension with the following data distribution: $w^*$ and $X$ are such that each $w^*_j$ and $x_{i,j}$ is independent and follow $\\mathcal{N}(0,1/j^2)$ (centered normal with variance $1/j^2$). Then we have a linear model $Y= X^T w^* + Z$ where $Z\\sim \\mathcal{N}(0,\\sigma^2)$ (for simplicity, we omit the intercept/bias and consider linear predictors instead of affine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we have shown that the fixed design risk of the OLS estimator $R_X(\\hat w)$ has an expectation of $\\sigma^2d/n$ (where the expectation is with respect to the random draw of the training outputs $Y_i$). Here we will measure the true risk, with a random design, and see if the behavior is similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) Fix $d=20$ and $\\sigma = 0.5$ and plot the excess risk $R(\\hat w) - R^*= \\mathbb{E}_{X}[(X^T(\\hat{w}-w^*)) | w^*] = \\Vert \\hat{w} - w^*\\Vert_\\Sigma^2$ as a function of $n$ for $n\\geq d+1$, where $\\Sigma = \\mathbb{E}[XX^T] = \\mathrm{diag}(1/j^2)$. Does it converge to $0$? At what rate? (you may need to average over several realizations of $\\hat w$ to see a clear trend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge least-squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11) In the same setting, compute the ridge least-squares estimator with a regularization parameter $\\lambda$. Plot the excess risk (as above) as a function of $\\lambda$ for $\\sigma =0.5$, $n=50$ and $d=20$ and $\\lambda \\in [0,10]$. Plot the same for $d=100$. Is the excess risk smaller than without regularization? At the best value for $\\lambda$, is there an effect of increasing the dimension? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that with a proper choice of $\\lambda$ (which is in $O(1/\\sqrt{n})$) the (fixed design) excess risk of the ridge regression is in $O(1/\\sqrt{n})$. Let us see if we observe a similar behavior with a random design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(12) With $d=20$, plot the excess risk as a function of $n$ where for each value of $n$, the value of the regularization $\\lambda$ is chosen in the set $\\{ 2^{-2},2^{-1},2^0,2^1,2^2,2^3\\}$ as the one with the smallest excess risk. Compare with the curve obtained with OLS in the same setting above. Note that in practice, we do not have access to the excess risk and $\\lambda$ has to be chosen via cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. (Optional) Linear and polynomial classification on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset from the course website and save it into the folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('mnist_digits.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data['x']\n",
    "data_y = data['y']\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(data['x']))\n",
    "plt.imshow(np.reshape(data_x[i],(28,28)), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(13) Optional question: choose two digits and train a linear classifier that distinguishes the two digits. You may use linear least-squares regression, where $y_i \\in \\{-1,+1\\}$ depends on the digit. Taking the sign of the learnt predictor then gives a classifier.\n",
    "\n",
    "Make sure that you divide the dataset into training and test sets to be able to evaluate the performance of your algorithms. If your algorithm involves a regularization parameter, use a validation set (or cross-validation) to fix it.\n",
    "\n",
    "Add noise to the images and repeat the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
