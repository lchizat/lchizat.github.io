# jemdoc: analytics{UA-65279943-1}

~~~
{}{img_left}{files/photo.png}{photo de lenaic}{WIDTHpx}{220px}{files/photo.png}

= Lénaïc Chizat
I am a tenure track assistant professor at EPFL in the [https://www.epfl.ch/schools/sb/research/math/fr/ Institute of Mathematics], where I lead the DOLA chair (Dynamics Of Learning Algorithms). 

Main research topics:
- *Continuous Optimization*: gradient flows, algorithmic regularization, space of measures, interacting particle methods
- *Optimal Transport*: extensions, statistical aspects, entropic regularization, applications to machine learning
- *Theory of deep learning* and neural networks

To contact me : firstname.lastname@epfl.ch

~~~
{}{img_left}{files/email.png}{my email}{WIDTHpx}{20px}{files/email.png} 
~~~


== Publications
\[[https://scholar.google.fr/citations?user=jrJh9yIAAAAJ&hl=fr Google Scholar profile]\]

== Curriculum Vitae
\[[files/CHIZAT_CV.pdf  PDF]\]


== Master courses
- Spring 2021: [https://www.di.ens.fr/~fbach/ML_physique_2021.html Machine learning - Master ICFP - ENS]
- Spring 2021: [ot2021orsay.html Optimal Transport - Master Optimisation - Université Paris-Saclay]
- Spring 2020: [https://www.di.ens.fr/~fbach/ML_physique.html Machine learning - Master ICFP - ENS]
- Spring 2020: [ot2020orsay.html Optimal Transport - Master Optimisation - Université Paris-Saclay]

== Some resources
- Invited [https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/ blog post] on Francis Bach's research blog, an overview of our recent results on the predictor learnt by two-layer neural networks.
- [files/CHIZAT_SIAM_2022.pdf Slides] for the best paper award lecture at SIAM-IP 2022 about my research on sparse optimization on measures with overparamterized gradient descent.
- [files/CHIZAT_neurips_2021 Slides] about estimating optimal transport with entropic regularization
- [files/CHIZAT_wide_2021.pdf Slides] about infinitely wide two-layer neural networks
- All the experiments in my papers are reproducible using the codes I post [https://github.com/lchizat on Github]

== Miscellaneous
- My favorite programming language is [https://julialang.org/ Julia] and when I get a cool numerical animation I sometimes post it [https://twitter.com/LenaicChizat on Twitter].
- I also like to hear, play and create music, see eg [https://lukyou.bandcamp.com/album/dirty-dishes   a project with L. Dulac]
- Here are some great research blogs I like to follow [https://francisbach.com/ Francis Bach], [https://djalil.chafai.net/blog/ Djalil Chafaï], [https://twitter.com/gabrielpeyre Gabriel Peyré on Twitter], [https://terrytao.wordpress.com/ Terrence Tao]. On youtube, [https://www.youtube.com/c/3blue1brown 3blue1brown] is a must watch for any math student.
#- (July 2020) Talk for the workshop [https://www.otra2020.com/ Optimal Transport: Regularization and Applications] on [https://arxiv.org/abs/2006.08172 Faster Wasserstein Distance Estimation with the Sinkhorn Divergence]. \[[files/presentations/chizat200708otra.pdf slides]\] \[[https://columbia.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=37168c31-5a8a-4753-9750-abf2012546ba video]\]
#- (May, 2020) Talk at [https://www.msri.org/workshops/928 MSRI Hot Topic Workshop] on the "Analysis of Gradient Descent on Wide Two-layer Relu Neural Networks" \[[https://www.msri.org/workshops/928/schedules/28397 video]\] \[[files/presentations/chizat200508msri.pdf slides pdf]\]\[[files/presentations/chizat200508msri.zip slides w/videos zip]\]
#- 11/2019 *Tutorial on non-convex optimization with gradient methods (II).* (see [https://guillaume-garrigos.com/ Guillaume Garrigos webpage] for part (I)), SMILE in Paris seminar \[[files/presentations/chizat191128smile.pdf slides]\]
#- 11/2019 *Optimization on Measures with Over-parameterized Gradient Descent.* Optimization on Measures workshop, IMT Toulouse \[[files/presentations/chizat191126toulouse.pdf slides]\]
#- 11/2019 *Two analyses for of gradient-based optimization for wide two-layer neural networks.* Theory of Neural Networks seminar, EPFL \[[files/presentations/chizat191105epfl.pdf slides]\]
#- 07/2019 *Theoretical aspects of Neural Networks Optimization.* IISc Bangalore. \[[files/presentations/chizat2019IFCAM_NN.pdf slides]\] \[[files/presentations/chizat2019IFCAM_NN.zip download slides with animations]\]
#- 07/2019 *Tutorial on Optimal Transport with a Machine Learning Touch.* IISc Bangalore. \[[files/presentations/chizat2019IFCAM_OT.pdf slides]\] \[[files/presentations/chizat2019IFCAM_OT.zip download slides with animations]\]
#- 07/2018 *On the Global Convergence of Gradient Descent for Over-parameterized Models.* IFIP conference in Essen. \[[files/chizat2018ifip.zip download slides with animations]\]
#- 02/2018 *A Tutorial on Optimal Transport* with A. Genevay at [https://imaging-in-paris.github.io/seminar/ Imaging in Paris]. \[[files/CHIZAT_imagingParis_080218.pdf slides]\]
#- 11/2017 *Unbalanced Optimal Transport.* PhD Defense at Université Paris Dauphine. \[[files/CHIZAT_soutenance.pdf slides in French]\] \[[chizat2017phddefense.pdf slides in English]\]
#- 09/2017 *A Primer on Optimal Transport*, talk at the [https://www.broadinstitute.org/ Broad institute]'s [https://www.broadinstitute.org/scientific-community/science/mia/models-inference-algorithms MIA seminar] (MIT-Harvard). \[[https://www.youtube.com/watch?v=vJx7NiXFMi8 watch video]\].

