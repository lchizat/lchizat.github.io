# jemdoc: menu{MENU}{PGF.html}
= Particle gradient flows
 
~~~
{Preprint}
Lenaic Chizat, Francis Bach. On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport. 2018. 〈hal-01798792〉
~~~


We display on below animated versions of the particle gradient flows shown in Section 4 of the preprint, as well as a particle gradient flow corresponding to the training of a neural network with a single hidden layer and sigmoid activation function in dimension d=2. In all these cases, the global minimizer (supported on the dotted lines in the plots) is found. The time scale is logarithmic.

~~~
{}{img_left}{files/PGFspikes.gif}{spikes deconvolution}{WIDTHpx}{220px}{files/PGFspikes.gif}

Sparse spikes deconvolution, with position shown horizontally and weight shown vertically.
~~~

~~~
{}{img_left}{files/PGFrelu.gif}{ReLU activation}{WIDTHpx}{220px}{files/PGFrelu.gif}

Neural network with ReLU activation function: we show for each particle the trajectory $\vert w(t)\vert\cdot \theta(t)\in \mathbb{R}^2$.
~~~

~~~
{}{img_left}{files/PGFsigmoid.gif}{sigmoid activation}{WIDTHpx}{220px}{files/PGFsigmoid.gif}

neural network with sigmoid activation function, with weights represented by the size of the particles (blue for negative, red for positive, ground truth shown with 2 neurons shown by large disks)
~~~