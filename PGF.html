<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Particle gradient flows</title>
<!-- MathJax -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lénaïc Chizat</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="curriculum.html">Curriculum</a></div>
<div class="menu-item"><a href="talks.html">Talks&nbsp;&amp;&nbsp;Organization</a></div>
<div class="menu-item"><a href="https://github.com/lchizat">Code</a></div>
<div class="menu-category">Projects</div>
<div class="menu-item"><a href="PGF.html" class="current">Particle&nbsp;gradient&nbsp;flow</a></div>
<div class="menu-item"><a href="UOT.html">Unbalanced&nbsp;OT</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Particle gradient flows</h1>
</div>
<p>Many tasks in machine learning and signal processing require to minimize a convex function of a measure:</p>
<p style="text-align:center">
\[
\min_{\mu \in \mathcal{M}(\Theta)} J(\mu)
\]
</p><p>where \(\mathcal{M}(\Theta)\) is the space of measures on a manifold \(\Theta\). We focus on the case where \(J\) is (weakly) continuous and (Fréchet) differentiable. For this optimization problem, we investigate the method that consists in discretizing the measure into particles \(\mu = \sum_{i=1}^m w_i \delta_{\theta_i}\) and taking the gradient flow of \(J\) in the parameters \((w_i,\theta_i)_i\). We show that quite generically, this gradient flow converges to global minimizers in the many-particle limit \(m,t \to \infty\), under two main conditions:</p>
<ul>
<li><p>the differential of \(J\), seen has a continuous function on \(\Theta\), has some &ldquo;homogeneity directions&rdquo;, and </p>
</li>
<li><p>the initial distribution of particles is admissible.</p>
</li>
</ul>
<div class="infoblock">
<div class="blocktitle">Preprint</div>
<div class="blockcontent">
<p>Lenaic Chizat, Francis Bach. On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport. 2018. <a href="https://hal.archives-ouvertes.fr/hal-01798792" target=&ldquo;blank&rdquo;>&lt;hal-01798792&gt;</a></p>
</div></div>
<h3>Animations</h3>
<p>We display below animated versions of the particle gradient flows shown in Section 4 of the preprint, as well as a particle gradient flow corresponding to the training of a neural network with a single hidden layer and sigmoid activation function in dimension d=2. In all these cases, the global minimizer (supported on the dotted lines in the plots) is found. The time scale is logarithmic.</p>
<p><b>Sparse spikes deconvolution</b></p>
<table class="imgtable"><tr><td>
<a href="files/PGFspikes.gif"><img src="files/PGFspikes.gif" alt="spikes deconvolution" width="WIDTHpx" height="220px" /></a>&nbsp;</td>
<td align="left"><p>Here we solve the BLasso problem for recovering a sum-of-impulse signal given noisy and filtered observations. The positions \(\theta(t)\) of the particles are shown horizontally and weight \(w(t)\) shown vertically. This initialization is evenly spaced on \(\{0\} \times [0,1]\).</p>
</td></tr></table>
<p><b>Neural networks with a single hidden layer</b></p>
<p>We train such a network on a synthetic well-specified model with SGD.</p>
<table class="imgtable"><tr><td>
<a href="files/PGFrelu.gif"><img src="files/PGFrelu.gif" alt="ReLU activation" width="WIDTHpx" height="220px" /></a>&nbsp;</td>
<td align="left"><p>ReLU activation function: we show for each particle the trajectory \(\vert w(t)\vert\cdot \theta(t)\in \mathbb{R}^2\). The initialization is on a sphere around \(0\), as our analysis suggests.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="files/PGFsigmoid.gif"><img src="files/PGFsigmoid.gif" alt="sigmoid activation" width="WIDTHpx" height="220px" /></a>&nbsp;</td>
<td align="left"><p>Sigmoid activation function: weights \(w(t)\) represented by the size of the particles (blue for negative, red for positive, ground truth shown with 2 neurons shown by large disks). The initialization is distributed according to a normal law on \(\mathbb{R}^2\).</p>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2018-05-24 19:56:34 CEST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
